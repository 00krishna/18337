---
title: The Different Flavors of Parallelism
author: Chris Rackauckas
date: September 22nd, 2019
---

Now that you are aware of the basics of parallel computing, let's give a high
level overview of the differences between different modes of parallelism.

## Lowest Level: SIMD

Recall SIMD, the idea that processors can run multiple commands simulataniously
on specially structured data. "Single Instruction Multiple Data". SIMD is
parallelism within a single core.

### How to do SIMD

The simplest way to do SIMD is simply to make sure that your values are aligned.
If they are, then great, LLVM's autovectorizer pass has a good chance of
automatic vectorization (in the world of computing, "SIMD" is synonymous with
vectorization since it is taking specific values and instead computing on
small vectors. That is not to be confused with "vectorization" in the sense
of Python/R/MATLAB, which is a programming style which prefers using C-defined
primitive functions, like broadcast or matrix multiplication).

You can check for auto-vectorization inside of the LLVM IR by looking for
statements like:

```
%wide.load24 = load <4 x double>, <4 x double> addrspac(13)* %46, align 8
; └
; ┌ @ float.jl:395 within `+'
%47 = fadd <4 x double> %wide.load, %wide.load24
```

which means that 4 additions are happening simultaniously. The amount of
vectorization is heavily dependent on your architecture. The ancient form of
SIMD, the SSE(2) instructions, required that your data was aligned. Now there's
a bit more leeway, but generally it holds that making your the data you're
trying to SIMD over is aligned. Thus there can be major differences in computing
using a *struct of array* format instead of an *arrays of structs* format. For
example:

```julia
struct MyComplex
  real::Float64
  imag::Float64
end
arr = [MyComplex(rand(),rand()) for i in 1:100]
```

is represented in memory as

```
[real1,imag1,real2,imag2,...]
```

while the struct of array formats are

```julia
struct MyComplexes
  real::Vector{Float64}
  imag::Vector{Float64}
end
arr2 = MyComplexes(rand(100),rand(100))
```

Now let's check what happens when we perform a reduction:

```julia
using InteractiveUtils
Base.:+(x::MyComplex,y::MyComplex) = MyComplex(x.real+y.real,x.imag+y.imag)
Base.:/(x::MyComplex,y::Int) = MyComplex(x.real/y,x.imag/y)
average(x::Vector{MyComplex}) = sum(x)/length(x)
@code_llvm average(arr)
```

vs

```julia
average(x::MyComplexes) = MyComplex(sum(x.real),sum(x.imag))/length(x.real)
@code_llvm average(arr2)
```

```julia
using BenchmarkTools
@btime average(arr)
```

```julia
@btime average(arr2)
```

While this case is able to auto-vectorize (not all discontiguous calcuations can),
we can see that the in-memory structure matters for the efficiency and the struct
of array formulation is better in this case. Note it's because we are doing a
column-wise operation, i.e. averaging the reals and the imaginary parts separately.
If we were using each complex in isolation, then the struct of array formulation
would be better aligned to the calculation.

Note that we can explicitly force SIMD with the `@simd` macro. For example:

```julia
function not_forced_simd(x)
  out = 0.0
  for i in 1:100
    out += sqrt(x[i].real^2 + x[i].imag^2)
  end
  out
end
@code_llvm not_forced_simd(arr)
```

```julia
function forced_simd(x)
  out = 0.0
  @simd for i in 1:100
    out += sqrt(x[i].real^2 + x[i].imag^2)
  end
  out
end
@code_llvm forced_simd(arr)
```

```julia
@btime not_forced_simd(arr)
```

```julia
@btime forced_simd(arr)
```

It's different code but not realistically any faster. This is partly because
using "more" SIMD is actually quite hard. This amount of parallelism tends to
overheat the cores, and thus when the newest and largest SIMD is used (AVX2,
AVX512, etc.), these instructions actually require that the processor downclocks
for these interactions, somewhat offsetting the performance gain of doing multiple
calculations simulataniously. If enough are aligned in a row it still will give
a big speed up, but this means that there's a cost to doing too little of highly
compressed SIMD operations.

Note that in this case we are using
the real and imaginary parts together, so let's test our theory that the array
of structs formulation will be faster:

```julia
function SoA_forced_simd(x)
  out = 0.0
  @simd for i in 1:100
    out += sqrt(x.real[i]^2 + x.imag[i]^2)
  end
  out
end
@code_llvm SoA_forced_simd(arr2)
```

```julia
@btime SoA_forced_simd(arr2)
```

### Explicit SIMD

If you want to pack the vectors yourself, then primitives for doing so from
within Julia are available in SIMD.jl. This is for "real" performance warriors.

### Summary of SIMD

- Communication in SIMD is due to locality: if things are local the processor can
  automatically setup the operations.
- There's no real worry about "getting it wrong": you cannot overwrite pieces
  from different parts of the arithmetic unit, and if SIMD is unsafe then it just
  won't auto-vectorize.
- Suitable for operations measured in ns.

## Next Level Up: Multithreading

Last time we briefly went over multithreading and described how every process
has multiple threads which share a single heap, and when multiple threads are
executed simultaniously we have multithreaded parallelism. Note that you can
have multiple threads which aren't executed simultaniously, like in the case of
I/O operations, and this is an example of concurrency without parallelism and
is commonly referred to as green threads.

![](https://blog-assets.risingstack.com/2017/02/kernel-processes-and-threads-1.png)

Last time we described a simple multithreaded program and noticed that multithreading
has an overhead cost of around 50ns-100ns. This is due to the construction of the
new stack (amont other things) each time a new computational thread is spun up.
This means that, unlike SIMD, some thought needs to be put in as to when to
perform multithreading: it's not always a good idea. It needs to be high enough
on the cost for this to be counter-balanced.

One abstraction that was glossed over was the memory access style. Before, we
were considering a single heap, or an UMA style:

![](https://software.intel.com/sites/default/files/m/2/0/4/e/d/39352-figure-1.jpg)

However, this is the case for all shared memory devices. For example, compute
nodes on the HPC tend to be "dual Xeon" or "quad Xeon", where each Xeon processor
is itself a multicore processor. But each processor on its own accesses its own
local caches, and thus one has to be aware that this is setup in a NUMA
(non-uniform memory access) manner:

![](https://software.intel.com/sites/default/files/m/2/d/c/b/2/39353-figure-2.jpg)

where there is a cache that is closer to the prcessor and a cache that is further
away. Care should be taken in this to localize the computation per thread,
otherwise a cost associated with the memory sharing will be hit (but all sharing
will still be automatic).

In this sense, interthread communication is naturally done through the heap:
if you want other threads to be able to touch a value, then you can simply place
it on the heap and then it'll be available. We saw this last time by how overlapping
computations can re-use the same heap-based caches, meaning that care needs to
be taken with how one writes into a dynamically-allocated array.

A simple example that demonstrates this is:

```julia
using Base.Threads
acc = 0
@threads for i in 1:10_000
    global acc
    acc += 1
end
acc
```

The reason for this behavior is that there is a difference between the reading
and the writing step to an array. Here, values are being read while other threads
are writing, meaning that they see a lower value than when they are attempting
to write into it. The result is that the total summation is lower than the true
value because of this clashing. We can prevent this by only allowing one thread
to utilize the heap-allocated variable at a time. One abstraction for doing this
is *atomics*:

```julia
acc = Atomic{Int64}(0)
@threads for i in 1:10_000
    atomic_add!(acc, 1)
end
acc
```

When an atomic add is being done, all other threads wishing to do the same
computation are blocked. This of course can have a massive effect on performance
since atomic computations are not parallel.

Julia also exposes a lower level of heap control in threading using *locks*

```julia
const acc_lock = Ref{Int64}(0)
const mlock = Mutex()
function f1()
    @threads for i in 1:10_000
        lock(mlock)
        acc_lock[] += 1
        unlock(mlock)
    end
end
const splock = SpinLock()
function f2()
    @threads for i in 1:10_000
        lock(splock)
        acc_lock[] += 1
        unlock(splock)
    end
end
const rsplock = RecursiveSpinLock()
function f3()
    @threads for i in 1:10_000
        lock(rsplock)
        acc_lock[] += 1
        unlock(rsplock)
    end
end
acc2 = Atomic{Int64}(0)
function g()
  @threads for i in 1:10_000
      atomic_add!(acc2, 1)
  end
end
const acc_s = Ref{Int64}(0)
function h()
  global acc_s
  for i in 1:10_000
      acc_s[] += 1
  end
end
@btime f1()
```

`Mutex` is the most capable: just put locks and unlocks anywhere and it will work,
but it is costly. `SpinLock` is non-reentrent, i.e. it will block itself if a
thread that calls a `lock` does another `lock`. Therefore it has to be used
with caution (every `lock` goes with one `unlock`), but it's fast:

```julia
@btime f2()
```

Additionally there is the undocumented `RecursiveSpinLock` which allows a single
thread to recursively lock and then unlock, but if multiple threads try to
recursively lock it can run into issues. But it only has a small overhead:

```julia
@btime f3()
```

But if you can use atomics, they will be faster:

```julia
@btime g()
```

and if your computation is actually serial, then use serial code:

```julia
@btime h()
```

Why is this so fast? Check the code:

```julia
@code_llvm h()
```

It just knows to add 10000. So to get a proper timing let's make the size mutable:

```julia
const len = Ref{Int}(10_000)
function h2()
  global acc_s
  global len
  for i in 1:len[]
      acc_s[] += 1
  end
end
@btime h2()
```

```julia
@code_llvm h2()
```

It's still optimizing it!

```julia
non_const_len = 10000
function h3()
  global acc_s
  global non_const_len
  len2::Int = non_const_len
  for i in 1:len2
      acc_s[] += 1
  end
end
@btime h3()
```

Note that what is shown here is a type-declaration. `a::T = ...` forces `a` to
be of type `T` throughout the whole function. By giving the compiler this
information, I am able to use the non-constant global in a type-stable manner.

One last thing to note about multithreaded computations, and parallel computations,
is that one cannot assume that the parallelized computation is computed in any
given order. For example, the following will has a quasi-random ordering:

```julia
const a2 = zeros(nthreads()*10)
const acc_lock2 = Ref{Int64}(0)
const splock2 = SpinLock()
function f_order()
    @threads for i in 1:length(a2)
        lock(splock2)
        acc_lock2[] += 1
        a2[i] = acc_lock2[]
        unlock(splock2)
    end
end
f_order()
a2
```

Note that here we can see that Julia 1.1 is dividing up the work into groups of
10 for each thread, and then one thread dominates the computation at a time,
but which thread dominates is random.

### The Dining Philosophers Problem

A classic tale in parallel computing is the dining philosophers problem. In this
case, there are N philosophers at a table who all want to eat at the same time,
following all of the same rules. Each philosopher must alternatively think and
then eat. They need both their left and right fork to start eating, but cannot
start eating until they have both forks. The problem is how to setup a concurrent
algorithm that will not cause any philosophers to starve.

The difficulty is a situation known as *deadlock*. For example, if each philosopher
was told to grab the right fork when it's avaialble, and then the left fork, and
put down the fork after eating, then they will all grab the right fork and none
will ever eat because they will all be waiting on the left fork. This is analygous
to two blocked computations which are waiting on the other to finish. Thus, when
using blocking structures, one needs to be careful about deadlock!

### Summary of Multithreading

- Communication in multithreading is done on the heap. Locks and atomics allow
  for a form of safe message passing.
- 50ns-100ns of overhead. Suitable for 1μs calculations.
- Be careful of ordering and heap-allocated values.

## GPU Computing

GPUs are not fast. In fact, the problem with GPUs is that each processor is slow.
However, GPUs have a lot of cores... like thousands.

![](https://miro.medium.com/max/832/0*xzPjWMqXC0NB6D69.jpg)

An RTX2080, a standard "gaming" GPU (not even the ones in the cluster), has 2944
cores. However, not only are GPUs slow, but they also need to be programmed in
a style that is *SPMD*, which standard for Single Program Multiple Data. This
means that every single thread must be running the same program but on different
pieces of data. Exactly the same program. If you have

```julia;eval=false
if a > 1
  # Do something
else
  # Do something else
end
```

where some of the data goes on one branch and other data goes on the other
branch, every single thread will run both branches (performing "fake" computations
while on the other branch). This means that GPU tasks should be "very parallel"
with as few conditionals as possible.

This can be very difficult to actually use effectively, so for the most part
GPUs are used as array-operation accelerators. CuArrays.jl has a CUDA-accelerated
BLAS for high-performance linear algebra. If one wants to generate GPU code from
Julia, one can use CudaNative.jl or GPUifyLoops.jl, which will be a topic for
later in the course.

### GPU Memory

GPUs themselves are shared memory devices, meaning they have a heap that is
shared amongst all threads. However, GPUs are heavily in the NUMA camp, where
different blocks of the GPU have much faster access to certain parts of the
memory. Additionally, this heap is disconnected from the standard processor,
so data must be passed to the GPU and data must be returned.

GPU memory size is relatively small compared to CPUs. Example: the RTX2080Ti has
8GB of RAM. Thus one needs to be doing computations that are memory compact
(such as matrix multiplications, which are O(n^3) making the computation time
scale quicker than the memory cost).

### Note on GPU Hardware

Standard GPU hardware "for gaming", like RTX2070, is just as fast as higher end
GPU hardware for Float32. Higher end hardware, like the Tesla, add more memory,
memory safety, and Float64 support. However, these require being in a server
since they have alternative cooling strategies, making them a higher end
product.

### GPU Computing Example with CuArrays

```julia
A = rand(100,100); B = rand(100,100)
using CuArrays
# Pass to the GPU
cuA = cu(A); cuB = cu(B)
cuC = cuA*cuB
# Pass to the CPU
C = Array(cuC)
```

Let's see the transfer times:

```julia
@btime cu(A)
```

```julia
@btime Array(cuC)
```

The cost transferring is about 50μs-100μs in each direction, meaning that one
needs to be doing operations that cost at least 200μs for GPUs to break even.
A good rule of thumb is that GPU computations should take at least a milisecond,
or GPU memory should be re-used.

### Summary of GPUs

- GPUs cores are slow
- GPUs are SPMD
- GPUs are generally used for linear algebra
- Suitable for SPMD 1ms computations

## Xeon Phi Accelerators and OpenCL

Other architectures exist to keep in mind. Xeon Phis are a now-defunct accelerator
that used X86 (standard processors) as the base, using hundreds of them. For
example, the Knights Landing series had 256 core accelerator cards. These were
all clocked down, meaning they were still slower than a standard CPU, but there
were less restrictions on SPMD (though SPMD-like computations were still preferred
in order to heavily make use of SIMD). However, because machine learning
essentially only needs linear algebra, and linear algebra is faster when restricting
to SPMD-architectures, this failed. These devices can still be found on many
high end clusters.

One alternative to CUDA is OpenCL which supports alternative architectures such
as the Xeon Phi at the same time that it supports GPUs. However, one of the
issues with OpenCL is that its BLAS implementation currently does not match
the speed of CuBLAS, which makes NVIDIA-specific libraries still the king of
machine learning and most scientific computing.

## TPU Computing

TPUs are tensor processing units, which is Google's newest accelerator
technology. They are essentially just "tensor operation compilers", which in
computer science speak is simply higher dimensional linear algebra. To do this,
they internally utilize a BFloat16 type, which is a 16-bit floating point number
with the same exponent size as a Float32 with an 8-bit significand. This means
that computations are highly prone to *catastrophic cancellation*. This
computational device only works because BFloat16 has primitive operations for
FMA which allows 32-bit-like accuracy of multiply-add operations, and thus
computations which are only dot products (linear algebra) end up okay. Thus
this is simply a GPU-like device which has gone further to completely specialize
in linear algebra.

## Multiprocessing (Distributed Computing)

While multithreading computes with multiple threads, multiprocessing computes
with multiple independent processes. Note that processes do not share any memory,
not heap or data, and thus this mode of computing also allows for *distributed
computations*, which is the case where processes may be on separate computing
hardware. However, even if they are on the same hardware, the lack of a shared
address space means that multiprocessing has to do *message passing*, i.e.
send data from one process to the other.

### The Master-Worker Model

Given the amount of control over data handling, there are many different models
for distributed computing. The simplest, the one that Julia's Distributed
Standard Library defaults to, is the *master-worker model*. The master-worker
model has one process, deemed the master, which controls the worker processes.

Here we can start by adding some new worker processes:

```
using Distributed
addprocs(4)
```

This adds 4 worker processes for the master to control. The simplest computations
are those where the master process gives the worker process a job which returns
the value afterwards. For example, a `pmap` operation or `@distributed` loop
gives the worker a function to execute, along with the data, and the worker
then computes and returns the result.

At a lower level, this is done by `Distributed.@spawn`ing jobs, or using a
`remotecall` and `fetch`ing the result.
[ParallelDataTransfer.jl](https://github.com/ChrisRackauckas/ParallelDataTransfer.jl)
gives an extended set of primitive message passing operations.

### SharedArrays, Elemental, and DArrays

Because array operations are a standard way to compute in scientific computing,
there are higher level primitives to help with message passing. A `SharedArray`
is an array which acts like a shared memory device. This means that every change
to a `SharedArray` causes message passing to keep them in sync, and thus this
should be used with a performance caution.
[DistributedArrays.jl](https://github.com/JuliaParallel/DistributedArrays.jl) is
a parallel array type which has local blocks and can be used for writing
higher level abstractions with explicit message passing. Because it is currently
missing high-level parallel linear algebra, currently the recommended tool for
distributed linear algebra is [Elemental.jl](https://github.com/JuliaParallel/Elemental.jl).

### MapReduce, Hadoop, and Spark: The Map-Reduce Model

Many data-parallel operations work by mapping a function `f` onto each piece of
data and then reducing it. For example, the sum of squares maps the function
`x -> x^2` onto each value, and then these values are reduced by performing a
summation. MapReduce was a Google framework in the 2000's built around this
as the parallel computing concept, and current data-handling frameworks, like
Hadoop and Spark, continue this as the core distributed programming model.

In Julia, there exists the `mapreduce` function for performing serial mapreduce
operations. It also work on GPUs. However, it does not auto-distribute. For
distributed map-reduce programming, the `@distributed` for-loop macro can be used.
For example, sum of squares of random numbers is:

```
@distributed (+) for i in 1:1000
  rand()^2
end
```

One can see that computing summary statistics is easily done in this framework
which is why it was majorly adopted among "big data" communities.

### MPI

The main way to do high-performace multiprocessing is *MPI*, which is an old
distributed computing interface from the C/Fortran days. Julia has access to
the MPI programming model through MPI.jl which is installed on the Supercloud.
The programming model for MPI is that every computer is running the same program,
and synchronization is performed by blocking communication. For example, the
following prints on every process:

```
using MPI
MPI.Init()

comm = MPI.COMM_WORLD
print("Hello world, I am rank $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\n")
MPI.Barrier(comm)
```

```
> mpiexec -n 3 julia examples/01-hello.jl
Hello world, I am rank 0 of 3
Hello world, I am rank 1 of 3
Hello world, I am rank 2 of 3
```

We will go into detail on using MPI later in the course. This model can be faster
than the master-worker model because less communication to a single master is
required, but care has to be taken to make sure the computation doesn't deadlock.

### Summary of Multiprocessing

- Cost is hardware dependent: only suitable for 1ms or higher depending on the
  connections through which the messages are being passed and the topology of
  the network.
- Master-worker is Julia's model
- Map-reduce is a common data-handling model
- Array-based distributed computations are another abstraction
- MPI is the lowest abstract, where each process is completely independent and
  one just controls the memory handling.
