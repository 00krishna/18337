---
title: Differentiable Programming and Neural Differential Equations
author: Chris Rackauckas
date: October 15th, 2019
---

Our last discussion focused on how, at a high mathematical level, one could
in theory build programs which compute gradients in a fast manner by looking
at the computational graph and performing reverse-mode automatic differentiation.
Within the context of parameter identification, we saw many advantages to this
approach because it did not scale multiplicatively in the number of parameters,
and thus it is an efficient way to calculate Jacobians of objects where there
are less rows than columns (think of the gradient as 1 row).

More precisely, this is seen to be more about sparsity patterns, with
reverse-mode as being more efficient if there are "enough" less row seeds
required than column partials (with mixed mode approaches sometimes being much
better). However, to make reverse-mode AD realistically usable inside of a
programming language instead of a compute graph, we need to do three things:

1. We need to have a way of implementing reverse-mode AD on a language.
2. We need a systematic way to derive "adjoint" relationships (pullbacks).
3. We need to see if there are better ways to fit parameters to data, rather
   than performing reverse-mode AD through entire programs!

## Implementation of Reverse-Mode AD

Forward-mode AD was implementable through operator overloading and dual number
arithmetic. However, reverse-mode AD requires reversing a program through
its computational structure, which is a much more difficult operation. This begs
the question, how does one actually build a reverse-mode AD implementation?

### Static Graph AD

The most obvious solution is to use a static compute graph, since how we
defined our differentiation structure was on a compute graph. Tensorflow is a
modern example of this approach, where a user must define variables and
operations in a graph language (that's embedded into Python, R, Julia, etc.),
and then execution on the graph is easy to differentiate. This has the advantage
of being a simplified and controlled form, which means that not only differetiation
tranformations are possible, but also things like automatic parallelization.
However, many see directly writing a (static) computation graph as a barrier
for practical use since it requires completely rewriting all existing programs
to this structure.

### Tracing-Based AD and Wengert Lists

Recall that an alternative formulation of reverse-mode AD for composed functions

$$f = f^L \circ f^{L-1} \circ \ldots \circ f^1$$

is through pullbacks on the Jacobians:

$$v^T J = (\ldots ((v^T J_L) J_{L-1}) \ldots ) J_1$$

Therefore, if one can transform the program structure into a list of composed
functions, then reverse-mode AD is the successive application of pullbacks
going in the reverse direction:

\mathcal{B}_{f}^{x}(A)=\mathcal{B}_{f^{1}}^{x}\left(\ldots\left(\mathcal{\mathcal{B}}_{f^{L-1}}^{f^{L-2}(f^{L-3}(\ldots f^{1}(x)\ldots))}\left(\mathcal{B}_{f^{L}}^{f^{L-1}(f^{L-2}(\ldots f^{1}(x)\ldots))}(A)\right)\right)\ldots\right)

Recall that the pullback $\mathcal{B}_f^x(\overline{y})$ requires knowing:

1. The operation being performed
2. The value $x$ of the forward pass

The idea is to then build a *Wengert list* that is from exactly the forward
pass of a specific $x$, also known as a *trace*, and thus giving rise to
*tracing-based reverse-mode AD*. This is the basis of many reverse-mode
implementations, such as Julia's Tracker.jl (Flux.jl's old AD), ReverseDiff.jl,
PyTorch, Tensorflow Eager, Autograd, and Autograd.jl. It is widely adopted due
to its simplicity in implementation.

#### Inspecting Tracker.jl

Tracker.jl is a very simple implementation to inspect. The definition of its
number and array types are as follows:

```julia;eval=false
struct Call{F,As<:Tuple}
  func::F
  args::As
end

mutable struct Tracked{T}
  ref::UInt32
  f::Call
  isleaf::Bool
  grad::T
  Tracked{T}(f::Call) where T = new(0, f, false)
  Tracked{T}(f::Call, grad::T) where T = new(0, f, false, grad)
  Tracked{T}(f::Call{Nothing}, grad::T) where T = new(0, f, true, grad)
end

mutable struct TrackedReal{T<:Real} <: Real
  data::T
  tracker::Tracked{T}
end

struct TrackedArray{T,N,A<:AbstractArray{T,N}} <: AbstractArray{T,N}
  tracker::Tracked{A}
  data::A
  grad::A
  TrackedArray{T,N,A}(t::Tracked{A}, data::A) where {T,N,A} = new(t, data)
  TrackedArray{T,N,A}(t::Tracked{A}, data::A, grad::A) where {T,N,A} = new(t, data, grad)
end
```

As expected, it replaces every single number and array with a value that will
store not just perform the operation, but also build up a list of operations
along with the values at every stage. Then pullback rules are implemented for
primitives via the `@grad` macro. For example, the pullback for the dot product
is implemented as:

```julia;eval=false
@grad dot(xs, ys) = dot(data(xs), data(ys)), Δ -> (Δ .* ys, Δ .* xs)
```

This is read as: the value going forward is computed by using the Julia `dot`
function on the arrays, and the pullback embeds the backs of the forward pass
and uses `Δ .* ys` as the derivative with respect to `x`, and `Δ .* xs` as the
derivative with respect to `y`. This element-wise nature makes sense given the
diagonal-ness of the Jacobian.

Note that this also allows utilizing intermediates of the forward pass within
the reverse pass. This is seen in the definition of the pullback of `meanpool`:

```julia;eval=false
@grad function meanpool(x, pdims::PoolDims; kw...)
  y = meanpool(data(x), pdims; kw...)
  y, Δ -> (nobacksies(:meanpool, NNlib.∇meanpool(data.((Δ, y, x))..., pdims; kw...)), nothing)
end
```

where the derivative makes use of not only `x`, but also `y` so that the `meanpool`
does not need to be re-calculated.

Using this style, Tracker.jl moves forward, building up the value and closures
for the backpass and then recursively pulls back the input `Δ` to receive the
derivatve.

### Source-to-Source AD

Given our previous discussions on performance, you should be horrified with
how this approach handles scalar values. Each `TrackedReal` holds as
`Tracked{T}` which holds a `Call`, not a `Call{F,As<:Tuple}`, and thus it's not
strictly typed. 

## Derivation of Reverse Mode Rules: Adjoints and Implicit Function Theorem

### Adjoint of Linear Solve

### Adjoint of Nonlinear Solve

### Adjoint of Ordinary Differential Equations

### Complexities of Implementing ODE Adjoints

### Neural Ordinary Differential Equations

## Alternative "Training" Strategies

### Multiple Shooting Techniques

### Collocation Methods
